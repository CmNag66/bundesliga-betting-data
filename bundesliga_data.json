import requests
from bs4 import BeautifulSoup
import json
import time
from datetime import datetime
import logging
import os
from typing import Dict, List, Optional
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

class GermanFootballScraper:
    def __init__(self):
        self.base_url = "https://fbref.com"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        # Verzeichnis für Daten
        self.data_dir = "german_football_data"
        if not os.path.exists(self.data_dir):
            os.makedirs(self.data_dir)

        # Logging einrichten
        logging.basicConfig(
            filename=f'{self.data_dir}/scraper.log',
            level=logging.DEBUG if os.getenv("DEBUG_MODE") else logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )

        # HTTP-Session mit Retry-Mechanismus
        self.session = requests.Session()
        retries = Retry(total=3, backoff_factor=5, status_forcelist=[429, 500, 502, 503, 504])
        self.session.mount("https://", HTTPAdapter(max_retries=retries))

        # Saisons definieren
        self.seasons = ["2024-2025", "2023-2024"]
        
        # Teams nach Saisons und Ligen
        self.teams_by_season = {
            "2024-2025": {
                "bundesliga": {
                    'bayern': {'id': '054efa67', 'name': 'Bayern München'},
                    'leverkusen': {'id': 'c7a9f859', 'name': 'Bayer Leverkusen'}
                    # Weitere Teams hier einfügen...
                }
            },
            "2023-2024": {
                "bundesliga": {
                    'bayern': {'id': '054efa67', 'name': 'Bayern München'},
                    'leverkusen': {'id': 'c7a9f859', 'name': 'Bayer Leverkusen'}
                    # Weitere Teams hier einfügen...
                }
            }
        }

    def get_team_data(self, team_id: str, season: str) -> Dict:
        """Erweiterte Teamdaten-Sammlung"""
        url = f"{self.base_url}/en/squads/{team_id}/{season}"
        try:
            response = self.session.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            team_data = {
                'general': self._extract_table_stats(soup, 'stats_standard'),
                'passing': self._extract_table_stats(soup, 'stats_passing_squads'),
                'defense': self._extract_table_stats(soup, 'stats_defense_squads')
            }
            return team_data
        except requests.RequestException as e:
            logging.error(f"Netzwerkfehler bei {url}: {e}")
        except Exception as e:
            logging.error(f"Fehler bei {url}: {e}")
        return {}

    def _extract_table_stats(self, soup, table_id: str) -> Dict:
        """Extrahiert Statistiken aus einer Tabelle."""
        stats = {}
        try:
            table = soup.find('table', {'id': table_id})
            if table:
                rows = table.find('tbody').find_all('tr')
                for row in rows:
                    cols = row.find_all(['th', 'td'])
                    if len(cols) >= 2:
                        stats[cols[0].text.strip()] = cols[1].text.strip()
        except Exception as e:
            logging.error(f"Fehler bei Tabelle {table_id}: {e}")
        return stats

    def collect_all_data(self) -> Dict:
        """Sammelt Daten aller Teams aus beiden Ligen und beiden Saisons."""
        all_data = {
            'last_update': datetime.now().isoformat(),
            'seasons': {}
        }
        for season in self.seasons:
            all_data['seasons'][season] = {'bundesliga': {}}
            for team_key, team_info in self.teams_by_season[season]['bundesliga'].items():
                logging.info(f"Sammle Daten für {team_info['name']} ({season})...")
                all_data['seasons'][season]['bundesliga'][team_key] = {
                    'name': team_info['name'],
                    'data': self.get_team_data(team_info['id'], season)
                }
                time.sleep(1)  # Kurze Verzögerung für Höflichkeit
                self.save_data(all_data)
        return all_data

    def save_data(self, data: Dict, filename: str = 'german_football_data.json'):
        """Speichert die Daten als JSON."""
        filepath = os.path.join(self.data_dir, filename)
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            logging.info(f"Daten wurden erfolgreich in {filepath} gespeichert.")
        except Exception as e:
            logging.error(f"Fehler beim Speichern der Daten: {e}")

if __name__ == "__main__":
    scraper = GermanFootballScraper()
    print("Starte Datensammlung...")
    all_data = scraper.collect_all_data()
    print("Datensammlung abgeschlossen!")
